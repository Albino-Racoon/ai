{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypandoc\n",
      "  Downloading pypandoc-1.13-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading pypandoc-1.13-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: pypandoc\n",
      "Successfully installed pypandoc-1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\jasar\\anaconda3\\lib\\site-packages\\databox-2.1.4-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\jasar\\ANACONDA3\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pypandoc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import time\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import openai\n",
    "import requests\n",
    "import shutil\n",
    "import docx\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning script started\n",
      "Loading data...\n",
      "Received 4 files.\n",
      "Downloaded file from https://firebasestorage.googleapis.com/v0/b/skillsbooster-a4155.appspot.com/o/daj_bog_da_dela%2F10_4_friedmanov_test.pdf?alt=media&token=780043cd-cc0f-4ee0-aab1-74eddbd80f85 to temp_files\\10_4_friedmanov_test.pdf.\n",
      "Downloaded file from https://firebasestorage.googleapis.com/v0/b/skillsbooster-a4155.appspot.com/o/daj_bog_da_dela%2FSpecifikacije_SkillsBooster.docx?alt=media&token=1d8f8002-de64-4506-9545-047e8fd6e590 to temp_files\\Specifikacije_SkillsBooster.docx.\n",
      "Skipping unsupported file type: .docx\n",
      "Downloaded file from https://firebasestorage.googleapis.com/v0/b/skillsbooster-a4155.appspot.com/o/daj_bog_da_dela%2Fmartin_martin_martin.txt?alt=media&token=1aba8d2f-1daa-489b-a66f-e3cb598fbb4e to temp_files\\martin_martin_martin.txt.\n",
      "Downloaded file from https://firebasestorage.googleapis.com/v0/b/skillsbooster-a4155.appspot.com/o/daj_bog_da_dela%2Foogway.jpg?alt=media&token=e09c9445-a580-4b11-9632-f63c9184c7fd to temp_files\\oogway.jpg.\n",
      "Skipping unsupported file type: .jpg\n",
      "Generated 5 samples.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import openai\n",
    "\n",
    "def extract_text_from_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def download_file(url, destination):\n",
    "    if os.path.isfile(url):\n",
    "        if os.path.abspath(url) == os.path.abspath(destination):\n",
    "            print(f\"Source and destination are the same for {url}. Skipping copy.\")\n",
    "        else:\n",
    "            shutil.copy(url, destination)\n",
    "            print(f\"Copied local file from {url} to {destination}.\")\n",
    "    elif url.startswith('http://') or url.startswith('https://'):\n",
    "        response = requests.get(url)\n",
    "        with open(destination, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded file from {url} to {destination}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid input: {url}. Must be a valid URL or a file path.\")\n",
    "    \n",
    "    return destination\n",
    "\n",
    "def main(temp_file_path):\n",
    "    print(\"Fine-tuning script started\")\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    with open(temp_file_path, 'r', encoding='utf-8') as file:\n",
    "        datoteke = json.load(file)\n",
    "    \n",
    "    print(f\"Received {len(datoteke)} files.\")\n",
    "\n",
    "    temp_dir = \"temp_files\"\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "\n",
    "    # Process files and extract content\n",
    "    vsebina_datotek = []\n",
    "    for datoteka in datoteke:\n",
    "        file_path = download_file(datoteka['url'], os.path.join(temp_dir, datoteka['ime']))\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if ext == '.txt':\n",
    "            vsebina_datotek.append(extract_text_from_txt(file_path))\n",
    "        elif ext == '.pdf':\n",
    "            vsebina_datotek.append(extract_text_from_pdf(file_path))\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file type: {ext}\")\n",
    "            continue\n",
    "\n",
    "    # Check if there is any content to fine-tune\n",
    "    if not vsebina_datotek:\n",
    "        print(\"No valid files to process.\")\n",
    "        return\n",
    "\n",
    "    # Prepare content for fine-tuning\n",
    "    celotna_vsebina = \"\\n\".join(vsebina_datotek)\n",
    "    deli_vsebine = [celotna_vsebina[i:i+1000] for i in range(0, len(celotna_vsebina), 1000)]\n",
    "\n",
    "    samples = []\n",
    "    for del_vsebine in deli_vsebine:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": del_vsebine}\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        generated_output = response.choices[0].message.content\n",
    "        questions_answers = generated_output.split(\"\\n\\n\")\n",
    "        for qa in questions_answers:\n",
    "            if ':' in qa:\n",
    "                question, answer = qa.split(':', 1)\n",
    "                if question.strip() and answer.strip():\n",
    "                    samples.append({\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"user\", \"content\": question.strip()},\n",
    "                            {\"role\": \"assistant\", \"content\": answer.strip()}\n",
    "                        ]\n",
    "                    })\n",
    "\n",
    "    print(f\"Generated {len(samples)} samples.\")\n",
    "\n",
    "    # Append samples to the sample_data.jsonl file\n",
    "    with open(\"sample_data.jsonl\", 'a', encoding='utf-8') as outfile:\n",
    "        for sample in samples:\n",
    "            json.dump(sample, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"temp.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-proj-IyaHdd8nZPq7k080U5Bdgk8G1H6ZgY4WmRl6dZ81BVqkFbPndDjFZ3Zqs8T3BlbkFJAn87sUMD7qT16fD_XzaEt48JjjBNnyo7r3hBBS5Xi1IwHM03jd0cYXbLYA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 9\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m client\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     12\u001b[0m   file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     13\u001b[0m   purpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tune\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jasar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"sample_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file ID: file-05kqmFrqnoA263uLMUL01PRr\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "OPENAI_API_KEY = \"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\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Function to upload the training file\n",
    "def upload_training_file(file_path):\n",
    "    try:\n",
    "        response = openai.files.create(\n",
    "            file=open(file_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "        )\n",
    "        file_id = response.id\n",
    "        print(f\"Uploaded file ID: {file_id}\")\n",
    "        return file_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your training file\n",
    "    training_file_path = \"sample_data.jsonl\"\n",
    "    \n",
    "    # Upload the training file\n",
    "    training_file_id = upload_training_file(training_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-H8j5ITb3I2brSlg0HLJKMmXM', created_at=1723733554, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-bhWuou7ty2AtvmE5fNppEq6z', result_files=[], seed=1120650800, status='validating_files', trained_tokens=None, training_file='file-05kqmFrqnoA263uLMUL01PRr', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "openai.fine_tuning.jobs.create(\n",
    "  training_file=\"file-05kqmFrqnoA263uLMUL01PRr\", \n",
    "  model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job created with ID: ftjob-3UKJIuFmYXUK4Z0s4e9XljUB\n",
      "Job Status: validating_files... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Job Status: running... waiting for completion.\n",
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::9wWLlFAq\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "OPENAI_API_KEY = \"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\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Create a fine-tuning job and save the ID of the fine-tuned model into a variable\n",
    "def create_fine_tuning_job():\n",
    "    response = openai.fine_tuning.jobs.create(\n",
    "        training_file=\"file-05kqmFrqnoA263uLMUL01PRr\", \n",
    "        model=\"gpt-4o-mini-2024-07-18\"\n",
    "    )\n",
    "    \n",
    "    # Assuming you want to store the job ID and eventually the model ID\n",
    "    job_id = response.id\n",
    "    print(f\"Fine-tuning job created with ID: {job_id}\")\n",
    "    \n",
    "    # You can monitor the job status and retrieve the model ID once it's available\n",
    "    return job_id\n",
    "\n",
    "def get_fine_tuned_model_id(job_id):\n",
    "    # This function would typically be called after some time when the job has completed.\n",
    "    response = openai.fine_tuning.jobs.retrieve(job_id)\n",
    "    \n",
    "    # Wait until the fine-tuning job has completed\n",
    "    while response.status not in [\"succeeded\", \"failed\"]:\n",
    "        print(f\"Job Status: {response.status}... waiting for completion.\")\n",
    "        time.sleep(30)  # Check every 30 seconds\n",
    "        response = openai.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "    # Retrieve the model ID if the job succeeded\n",
    "    if response.status == \"succeeded\":\n",
    "        id_modela = response.fine_tuned_model\n",
    "        print(f\"Fine-tuned model ID: {id_modela}\")\n",
    "        return id_modela\n",
    "    else:\n",
    "        print(\"Fine-tuning job failed.\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job_id = create_fine_tuning_job()\n",
    "    id_modela = get_fine_tuned_model_id(job_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_id=\"ft:gpt-4o-mini-2024-07-18:personal::9wWLlFAq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletion' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m         conversation\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: assistant_message})\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mchat_with_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m, in \u001b[0;36mchat_with_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL_ID,\n\u001b[0;32m     24\u001b[0m     messages\u001b[38;5;241m=\u001b[39mconversation\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Get the assistant's response\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m assistant_message \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00massistant_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Append the assistant's response to the conversation\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ChatCompletion' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def extract_text_from_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def download_file(url, destination):\n",
    "    if os.path.isfile(url):\n",
    "        if os.path.abspath(url) == os.path.abspath(destination):\n",
    "            print(f\"Source and destination are the same for {url}. Skipping copy.\")\n",
    "        else:\n",
    "            shutil.copy(url, destination)\n",
    "            print(f\"Copied local file from {url} to {destination}.\")\n",
    "    elif url.startswith('http://') or url.startswith('https://'):\n",
    "        response = requests.get(url)\n",
    "        with open(destination, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded file from {url} to {destination}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid input: {url}. Must be a valid URL or a file path.\")\n",
    "    \n",
    "    return destination\n",
    "\n",
    "def upload_training_file(file_path):\n",
    "    try:\n",
    "        response = openai.File.create(\n",
    "            file=open(file_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "        )\n",
    "        file_id = response['id']\n",
    "        print(f\"Uploaded file ID: {file_id}\")\n",
    "        return file_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def create_fine_tuning_job(training_file_id):\n",
    "    try:\n",
    "        response = openai.FineTune.create(\n",
    "            training_file=training_file_id,\n",
    "            model=\"gpt-3.5-turbo\"\n",
    "        )\n",
    "        job_id = response['id']\n",
    "        print(f\"Fine-tuning job created with ID: {job_id}\")\n",
    "        return job_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating fine-tuning job: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def get_fine_tuned_model_id(job_id):\n",
    "    try:\n",
    "        while True:\n",
    "            response = openai.FineTune.retrieve(job_id)\n",
    "            status = response['status']\n",
    "            print(f\"Job Status: {status}... waiting for completion.\")\n",
    "            if status in [\"succeeded\", \"failed\"]:\n",
    "                break\n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "\n",
    "        if status == \"succeeded\":\n",
    "            id_modela = response['fine_tuned_model']\n",
    "            print(f\"Fine-tuned model ID: {id_modela}\")\n",
    "            return id_modela\n",
    "        else:\n",
    "            print(\"Fine-tuning job failed.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving fine-tuned model ID: {e}\")\n",
    "        return None\n",
    "\n",
    "def main(temp_file_path):\n",
    "    print(\"Fine-tuning script started\")\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    with open(temp_file_path, 'r', encoding='utf-8') as file:\n",
    "        datoteke = json.load(file)\n",
    "    \n",
    "    print(f\"Received {len(datoteke)} files.\")\n",
    "\n",
    "    temp_dir = \"temp_files\"\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "\n",
    "    # Process files and extract content\n",
    "    vsebina_datotek = []\n",
    "    for datoteka in datoteke:\n",
    "        file_path = download_file(datoteka['url'], os.path.join(temp_dir, datoteka['ime']))\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if ext == '.txt':\n",
    "            vsebina_datotek.append(extract_text_from_txt(file_path))\n",
    "        elif ext == '.pdf':\n",
    "            vsebina_datotek.append(extract_text_from_pdf(file_path))\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file type: {ext}\")\n",
    "            continue\n",
    "\n",
    "    # Check if there is any content to fine-tune\n",
    "    if not vsebina_datotek:\n",
    "        print(\"No valid files to process.\")\n",
    "        return\n",
    "\n",
    "    # Prepare content for fine-tuning\n",
    "    celotna_vsebina = \"\\n\".join(vsebina_datotek)\n",
    "    deli_vsebine = [celotna_vsebina[i:i+1000] for i in range(0, len(celotna_vsebina), 1000)]\n",
    "\n",
    "    samples = []\n",
    "    for del_vsebine in deli_vsebine:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": del_vsebine}\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        generated_output = response.choices[0].message['content']\n",
    "        questions_answers = generated_output.split(\"\\n\\n\")\n",
    "        for qa in questions_answers:\n",
    "            if ':' in qa:\n",
    "                question, answer = qa.split(':', 1)\n",
    "                if question.strip() and answer.strip():\n",
    "                    samples.append({\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"user\", \"content\": question.strip()},\n",
    "                            {\"role\": \"assistant\", \"content\": answer.strip()}\n",
    "                        ]\n",
    "                    })\n",
    "\n",
    "    print(f\"Generated {len(samples)} samples.\")\n",
    "\n",
    "    # Append samples to the sample_data.jsonl file\n",
    "    with open(\"sample_data.jsonl\", 'a', encoding='utf-8') as outfile:\n",
    "        for sample in samples:\n",
    "            json.dump(sample, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    training_file_path = \"sample_data.jsonl\"\n",
    "    training_file_id = upload_training_file(training_file_path)\n",
    "    job_id = create_fine_tuning_job(training_file_id)\n",
    "    id_modela = get_fine_tuned_model_id(job_id)\n",
    "    print(id_modela)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use a relative path to the temp.json file\n",
    "    script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    temp_file_path = os.path.join(script_dir, 'temp.json')\n",
    "    main(temp_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
